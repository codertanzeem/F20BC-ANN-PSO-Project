{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "063P9WcR4lHS"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "\n",
        "# ---------- Activations (stable) ----------\n",
        "\n",
        "def sigmoid(x: np.ndarray) -> np.ndarray:\n",
        "    x = np.clip(x, -500, 500)\n",
        "    return 1.0 / (1.0 + np.exp(-x))\n",
        "\n",
        "\n",
        "def sigmoid_derivative(z: np.ndarray) -> np.ndarray:\n",
        "    s = sigmoid(z)\n",
        "    return s * (1.0 - s)\n",
        "\n",
        "\n",
        "def relu(x: np.ndarray) -> np.ndarray:\n",
        "    return np.maximum(0.0, x)\n",
        "\n",
        "\n",
        "def relu_derivative(z: np.ndarray) -> np.ndarray:\n",
        "    return (z > 0.0).astype(z.dtype)\n",
        "\n",
        "\n",
        "def softmax(z: np.ndarray, axis: int = -1) -> np.ndarray:\n",
        "    # Works for 1D or 2D inputs; subtract max for numerical stability\n",
        "    if z.ndim == 1:\n",
        "        z_max = np.max(z)\n",
        "        exps = np.exp(z - z_max)\n",
        "        return exps / np.sum(exps)\n",
        "    z_max = np.max(z, axis=axis, keepdims=True)\n",
        "    exps = np.exp(z - z_max)\n",
        "    return exps / np.sum(exps, axis=axis, keepdims=True)\n",
        "\n",
        "\n",
        "ACTIVATION_FUNCTIONS: Dict[str, callable] = {\n",
        "    \"sigmoid\": sigmoid,\n",
        "    \"relu\": relu,\n",
        "    \"softmax\": softmax,\n",
        "}\n",
        "\n",
        "ACTIVATION_DERIVATIVES: Dict[str, callable] = {\n",
        "    \"sigmoid\": sigmoid_derivative,\n",
        "    \"relu\": relu_derivative,\n",
        "    # Softmax derivative is not used explicitly with cross-entropy; handled in backward\n",
        "}\n",
        "\n",
        "\n",
        "# ---------- Losses (stable) ----------\n",
        "\n",
        "def mse_loss(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "    return float(np.mean((y_true - y_pred) ** 2))\n",
        "\n",
        "\n",
        "def binary_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-7) -> float:\n",
        "    y_pred_c = np.clip(y_pred, eps, 1.0 - eps)\n",
        "    return float(-np.mean(y_true * np.log(y_pred_c) + (1.0 - y_true) * np.log(1.0 - y_pred_c)))\n",
        "\n",
        "\n",
        "def categorical_cross_entropy(y_true: np.ndarray, y_pred: np.ndarray, eps: float = 1e-7) -> float:\n",
        "    # y_true expected one-hot\n",
        "    y_pred_c = np.clip(y_pred, eps, 1.0 - eps)\n",
        "    return float(-np.mean(np.sum(y_true * np.log(y_pred_c), axis=1)))\n",
        "\n",
        "\n",
        "# ---------- Neural Network ----------\n",
        "\n",
        "class NeuralNetwork:\n",
        "    \"\"\"\n",
        "    A clean NumPy MLP with mini-batch training, L2 regularization, and multiple losses/activations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        layer_sizes: List[int],\n",
        "        activations: List[str],\n",
        "        learning_rate: float = 0.01,\n",
        "        l2: float = 0.0,\n",
        "        init: str = \"auto\",  # 'auto', 'xavier', 'he'\n",
        "        seed: Optional[int] = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            layer_sizes: sizes per layer including input and output (e.g., [n_in, h1, h2, n_out])\n",
        "            activations: activation name for each non-input layer (len == len(layer_sizes)-1)\n",
        "            learning_rate: step size for gradient descent\n",
        "            l2: L2 regularization coefficient\n",
        "            init: 'auto' selects Xavier for sigmoid/softmax and He for ReLU; or force 'xavier'/'he'\n",
        "            seed: random seed for reproducibility\n",
        "        \"\"\"\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "\n",
        "        assert len(activations) == len(layer_sizes) - 1, \"activations must match number of layers-1\"\n",
        "\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.activation_names = activations\n",
        "        self.activations = [ACTIVATION_FUNCTIONS[a] for a in activations]\n",
        "        self.activation_derivatives = [ACTIVATION_DERIVATIVES.get(a, None) for a in activations]\n",
        "        self.num_layers = len(layer_sizes) - 1\n",
        "        self.learning_rate = learning_rate\n",
        "        self.l2 = l2\n",
        "\n",
        "        self.weights: List[np.ndarray] = []\n",
        "        self.biases: List[np.ndarray] = []\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            fan_in = layer_sizes[i]\n",
        "            fan_out = layer_sizes[i + 1]\n",
        "            act_name = activations[i]\n",
        "\n",
        "            chosen_init = init\n",
        "            if init == \"auto\":\n",
        "                if act_name in (\"relu\",):\n",
        "                    chosen_init = \"he\"\n",
        "                else:\n",
        "                    chosen_init = \"xavier\"\n",
        "\n",
        "            if chosen_init == \"he\":\n",
        "                scale = np.sqrt(2.0 / fan_in)\n",
        "            elif chosen_init == \"xavier\":\n",
        "                # Glorot normal (variance 1/fan_in) is a common simplification\n",
        "                scale = np.sqrt(1.0 / fan_in)\n",
        "            else:\n",
        "                raise ValueError(\"init must be 'auto', 'xavier', or 'he'\")\n",
        "\n",
        "            W = np.random.randn(fan_in, fan_out) * scale\n",
        "            b = np.zeros((fan_out,), dtype=W.dtype)\n",
        "            self.weights.append(W)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    # ----- Forward -----\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        weights: Optional[List[np.ndarray]] = None,\n",
        "        biases: Optional[List[np.ndarray]] = None,\n",
        "    ) -> Tuple[np.ndarray, List[np.ndarray], List[np.ndarray]]:\n",
        "        W = self.weights if weights is None else weights\n",
        "        b = self.biases if biases is None else biases\n",
        "\n",
        "        activations: List[np.ndarray] = [X]\n",
        "        pre_activations: List[np.ndarray] = []\n",
        "\n",
        "        a = X\n",
        "        for l in range(self.num_layers):\n",
        "            z = a @ W[l] + b[l]\n",
        "            pre_activations.append(z)\n",
        "            a = self.activations[l](z)\n",
        "            activations.append(a)\n",
        "\n",
        "        return a, activations, pre_activations\n",
        "\n",
        "    # ----- Backward -----\n",
        "\n",
        "    def _output_delta(\n",
        "        self,\n",
        "        y_true: np.ndarray,\n",
        "        a_L: np.ndarray,\n",
        "        z_L: np.ndarray,\n",
        "        loss: str,\n",
        "        last_act_name: str,\n",
        "    ) -> np.ndarray:\n",
        "        # BCE + sigmoid and CCE + softmax use simplified gradient: a_L - y_true\n",
        "        if loss in (\"bce\", \"binary_cross_entropy\") and last_act_name == \"sigmoid\":\n",
        "            return a_L - y_true\n",
        "        if loss in (\"cce\", \"categorical_cross_entropy\") and last_act_name == \"softmax\":\n",
        "            return a_L - y_true\n",
        "\n",
        "        # Otherwise include activation derivative (e.g., MSE with sigmoid, etc.)\n",
        "        deriv = self.activation_derivatives[-1]\n",
        "        if deriv is None:\n",
        "            raise ValueError(f\"No derivative defined for output activation '{last_act_name}' with '{loss}'\")\n",
        "        return (a_L - y_true) * deriv(z_L)\n",
        "\n",
        "    def backward(\n",
        "        self,\n",
        "        y_true: np.ndarray,\n",
        "        activations: List[np.ndarray],\n",
        "        pre_activations: List[np.ndarray],\n",
        "        loss: str,\n",
        "    ) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
        "        m = y_true.shape[0]\n",
        "        dW: List[np.ndarray] = [None] * self.num_layers  # type: ignore\n",
        "        db: List[np.ndarray] = [None] * self.num_layers  # type: ignore\n",
        "\n",
        "        a_L = activations[-1]\n",
        "        z_L = pre_activations[-1]\n",
        "        last_act_name = self.activation_names[-1]\n",
        "\n",
        "        delta = self._output_delta(y_true, a_L, z_L, loss, last_act_name)\n",
        "\n",
        "        for l in reversed(range(self.num_layers)):\n",
        "            a_prev = activations[l]\n",
        "            dW[l] = (a_prev.T @ delta) / m + (self.l2 * self.weights[l])\n",
        "            db[l] = np.sum(delta, axis=0) / m\n",
        "\n",
        "            if l > 0:\n",
        "                deriv = self.activation_derivatives[l - 1]\n",
        "                if deriv is None:\n",
        "                    raise ValueError(f\"No derivative for activation '{self.activation_names[l-1]}'\")\n",
        "                delta = (delta @ self.weights[l].T) * deriv(pre_activations[l - 1])\n",
        "\n",
        "        return dW, db\n",
        "\n",
        "    # ----- Parameters update -----\n",
        "\n",
        "    def update_parameters(self, dW: List[np.ndarray], db: List[np.ndarray]) -> None:\n",
        "        for l in range(self.num_layers):\n",
        "            self.weights[l] -= self.learning_rate * dW[l]\n",
        "            self.biases[l] -= self.learning_rate * db[l]\n",
        "\n",
        "    # ----- Loss wrapper -----\n",
        "\n",
        "    def compute_loss(self, y_true: np.ndarray, y_pred: np.ndarray, loss: str) -> float:\n",
        "        if loss in (\"mse\",):\n",
        "            base = mse_loss(y_true, y_pred)\n",
        "        elif loss in (\"bce\", \"binary_cross_entropy\"):\n",
        "            base = binary_cross_entropy(y_true, y_pred)\n",
        "        elif loss in (\"cce\", \"categorical_cross_entropy\"):\n",
        "            base = categorical_cross_entropy(y_true, y_pred)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown loss '{loss}'\")\n",
        "\n",
        "        # Add L2 penalty\n",
        "        if self.l2 > 0.0:\n",
        "            l2_sum = sum(np.sum(W * W) for W in self.weights)\n",
        "            base += 0.5 * self.l2 * l2_sum\n",
        "        return base\n",
        "\n",
        "    # ----- Training -----\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        X_train: np.ndarray,\n",
        "        y_train: np.ndarray,\n",
        "        epochs: int = 1000,\n",
        "        batch_size: Optional[int] = None,\n",
        "        loss: str = \"mse\",\n",
        "        verbose: bool = True,\n",
        "        validation_data: Optional[Tuple[np.ndarray, np.ndarray]] = None,\n",
        "    ) -> Dict[str, List[float]]:\n",
        "        X_train = np.asarray(X_train, dtype=np.float64)\n",
        "        y_train = np.asarray(y_train, dtype=np.float64)\n",
        "\n",
        "        # Ensure 2D y for consistent math\n",
        "        if y_train.ndim == 1:\n",
        "            y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "        n = X_train.shape[0]\n",
        "        if batch_size is None:\n",
        "            batch_size = n\n",
        "\n",
        "        history = {\"train_loss\": [], \"val_loss\": []}\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            idx = np.random.permutation(n)\n",
        "            X_shuf = X_train[idx]\n",
        "            y_shuf = y_train[idx]\n",
        "\n",
        "            epoch_loss = 0.0\n",
        "            batches = 0\n",
        "\n",
        "            for i in range(0, n, batch_size):\n",
        "                X_batch = X_shuf[i : i + batch_size]\n",
        "                y_batch = y_shuf[i : i + batch_size]\n",
        "\n",
        "                y_pred, acts, pre_acts = self.forward(X_batch)\n",
        "                batch_loss = self.compute_loss(y_batch, y_pred, loss)\n",
        "                dW, db = self.backward(y_batch, acts, pre_acts, loss)\n",
        "                self.update_parameters(dW, db)\n",
        "\n",
        "                epoch_loss += batch_loss\n",
        "                batches += 1\n",
        "\n",
        "            avg_loss = epoch_loss / max(1, batches)\n",
        "            history[\"train_loss\"].append(avg_loss)\n",
        "\n",
        "            if validation_data is not None:\n",
        "                X_val, y_val = validation_data\n",
        "                X_val = np.asarray(X_val, dtype=np.float64)\n",
        "                y_val = np.asarray(y_val, dtype=np.float64)\n",
        "                if y_val.ndim == 1:\n",
        "                    y_val = y_val.reshape(-1, 1)\n",
        "\n",
        "                val_pred, _, _ = self.forward(X_val)\n",
        "                val_loss = self.compute_loss(y_val, val_pred, loss)\n",
        "                history[\"val_loss\"].append(val_loss)\n",
        "\n",
        "            if verbose and (epoch % max(1, epochs // 10) == 0 or epoch == epochs - 1):\n",
        "                if validation_data is not None:\n",
        "                    print(f\"Epoch {epoch+1}/{epochs} - loss: {avg_loss:.6f} - val_loss: {val_loss:.6f}\")\n",
        "                else:\n",
        "                    print(f\"Epoch {epoch+1}/{epochs} - loss: {avg_loss:.6f}\")\n",
        "\n",
        "        return history\n",
        "\n",
        "    # ----- Inference -----\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        out, _, _ = self.forward(np.asarray(X, dtype=np.float64))\n",
        "        return out\n",
        "\n",
        "    def predict_classes(self, X: np.ndarray) -> np.ndarray:\n",
        "        out = self.predict(X)\n",
        "        last_act = self.activation_names[-1]\n",
        "        if last_act == \"sigmoid\":\n",
        "            return (out >= 0.5).astype(int)\n",
        "        if last_act == \"softmax\":\n",
        "            return np.argmax(out, axis=1)\n",
        "        # Fallback for linear/MSE use-cases\n",
        "        return out\n",
        "\n",
        "    def evaluate(self, X: np.ndarray, y: np.ndarray, loss: str = \"mse\") -> float:\n",
        "        y = np.asarray(y, dtype=np.float64)\n",
        "        if y.ndim == 1:\n",
        "            y = y.reshape(-1, 1)\n",
        "        y_pred = self.predict(X)\n",
        "        return self.compute_loss(y, y_pred, loss)"
      ]
    }
  ]
}
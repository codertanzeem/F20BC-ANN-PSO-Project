{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "def create_fitness_function(ann: ANN, X_train: np.ndarray, y_train: np.ndarray) -> Callable:\n",
        "    \"\"\"\n",
        "    Create fitness function that evaluates ANN performance\n",
        "\n",
        "    This function couples PSO to ANN:\n",
        "    - Takes parameter vector from PSO particle\n",
        "    - Sets ANN weights/biases using these parameters\n",
        "    - Evaluates ANN on training data\n",
        "    - Returns performance metric (MAE) as fitness\n",
        "\n",
        "    Args:\n",
        "        ann: The ANN architecture to evaluate\n",
        "        X_train: Training input features (n_samples, n_features)\n",
        "        y_train: Training target values (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "        Fitness function that PSO will optimize\n",
        "    \"\"\"\n",
        "    def fitness_function(parameter_vector: np.ndarray) -> float:\n",
        "        \"\"\"\n",
        "        Evaluate ANN with given parameters\n",
        "\n",
        "        Args:\n",
        "            parameter_vector: Flat array of all ANN weights and biases\n",
        "\n",
        "        Returns:\n",
        "            Mean Absolute Error (MAE) on training data\n",
        "            Lower values indicate better performance\n",
        "        \"\"\"\n",
        "        # Decode PSO particle position into ANN parameters\n",
        "        ann.set_parameters(parameter_vector)\n",
        "\n",
        "        # Make predictions on training data\n",
        "        predictions = ann.predict(X_train)\n",
        "\n",
        "        # Calculate Mean Absolute Error\n",
        "        mae = np.mean(np.abs(predictions.flatten() - y_train.flatten()))\n",
        "\n",
        "        return mae\n",
        "\n",
        "    return fitness_function\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "def load_and_split_data(filepath: str, train_ratio: float = 0.7,\n",
        "                        random_seed: int = 42) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Load concrete strength dataset and split into train/test sets\n",
        "\n",
        "    Args:\n",
        "        filepath: /content/concrete_data.csv\n",
        "        train_ratio: Proportion of data for training (default 0.7 = 70%)\n",
        "        random_seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (X_train, X_test, y_train, y_test)\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from: {filepath}\")\n",
        "\n",
        "    # Load CSV file\n",
        "    data = pd.read_csv(/content/concrete_data.csv)\n",
        "    print(f\"  Dataset shape: {data.shape}\")\n",
        "\n",
        "    # Separate features (first 8 columns) and target (last column)\n",
        "    X = data.iloc[:, :-1].values  # All columns except last\n",
        "    y = data.iloc[:, -1].values   # Last column (compressive strength)\n",
        "\n",
        "    print(f\"  Features shape: {X.shape}\")\n",
        "    print(f\"  Target shape: {y.shape}\")\n",
        "\n",
        "    # Normalize features (important for neural networks)\n",
        "    # Standardization: (x - mean) / std\n",
        "    X_mean = np.mean(X, axis=0)\n",
        "    X_std = np.std(X, axis=0)\n",
        "    X = (X - X_mean) / (X_std + 1e-8)  # Add small value to prevent division by zero\n",
        "\n",
        "    print(f\"  Features normalized (mean=0, std=1)\")\n",
        "\n",
        "    # Shuffle data for random train/test split\n",
        "    np.random.seed(random_seed)\n",
        "    indices = np.random.permutation(len(X))\n",
        "    X = X[indices]\n",
        "    y = y[indices]\n",
        "\n",
        "    # Split into training and testing sets\n",
        "    split_idx = int(len(X) * train_ratio)\n",
        "    X_train = X[:split_idx]\n",
        "    X_test = X[split_idx:]\n",
        "    y_train = y[:split_idx]\n",
        "    y_test = y[split_idx:]\n",
        "\n",
        "    print(f\"  Training samples: {len(X_train)}\")\n",
        "    print(f\"  Testing samples: {len(X_test)}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "def evaluate_mae(ann: ANN, X: np.ndarray, y: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Calculate Mean Absolute Error of ANN predictions\n",
        "\n",
        "    MAE = (1/n) * Î£|predicted - actual|\n",
        "\n",
        "    Args:\n",
        "        ann: Trained ANN\n",
        "        X: Input features (n_samples, n_features)\n",
        "        y: True output values (n_samples,)\n",
        "\n",
        "    Returns:\n",
        "        Mean Absolute Error (lower is better)\n",
        "    \"\"\"\n",
        "    predictions = ann.predict(X)\n",
        "    mae = np.mean(np.abs(predictions.flatten() - y.flatten()))\n",
        "    return mae\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "def run_single_experiment(X_train, X_test, y_train, y_test,\n",
        "                         layer_sizes, activation_functions,\n",
        "                         swarm_size, max_iterations, bounds,\n",
        "                         num_informants=3, phi1=2.05, phi2=2.05, phi3=0.729,\n",
        "                         random_seed=None):\n",
        "    \"\"\"\n",
        "    Run a single PSO-ANN experiment\n",
        "\n",
        "    Args:\n",
        "        Data parameters:\n",
        "            X_train, X_test, y_train, y_test: Dataset splits\n",
        "        ANN parameters:\n",
        "            layer_sizes: List of neurons per layer\n",
        "            activation_functions: List of activation function names\n",
        "        PSO parameters:\n",
        "            swarm_size: Number of particles\n",
        "            max_iterations: Number of PSO iterations\n",
        "            bounds: (min, max) for parameters\n",
        "            num_informants: Number of informants per particle\n",
        "            phi1, phi2, phi3: PSO coefficients\n",
        "        random_seed: For reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "        random.seed(random_seed)\n",
        "\n",
        "    # Create ANN\n",
        "    ann = ANN(layer_sizes, activation_functions)\n",
        "    num_parameters = ann.get_parameter_count()\n",
        "\n",
        "    print(f\"\\nANN Architecture: {layer_sizes}\")\n",
        "    print(f\"Activation Functions: {activation_functions}\")\n",
        "    print(f\"Total Parameters: {num_parameters}\")\n",
        "\n",
        "    # Create fitness function\n",
        "    fitness_function = create_fitness_function(ann, X_train, y_train)\n",
        "\n",
        "    # Create and run PSO\n",
        "    print(f\"\\nPSO Configuration:\")\n",
        "    print(f\"  Swarm Size: {swarm_size}\")\n",
        "    print(f\"  Iterations: {max_iterations}\")\n",
        "    print(f\"  Total Evaluations: {swarm_size * max_iterations}\")\n",
        "    print(f\"  Bounds: {bounds}\")\n",
        "    print(f\"  Coefficients: phi1={phi1}, phi2={phi2}, phi3={phi3}\")\n",
        "\n",
        "    pso = PSO(\n",
        "        swarm_size=swarm_size,\n",
        "        dimension=num_parameters,\n",
        "        bounds=bounds,\n",
        "        num_informants=num_informants,\n",
        "        phi1=phi1,\n",
        "        phi2=phi2,\n",
        "        phi3=phi3\n",
        "    )\n",
        "\n",
        "    print(\"\\nStarting PSO optimization...\")\n",
        "    best_parameters, best_fitness = pso.optimize(fitness_function, max_iterations)\n",
        "\n",
        "    # Set best parameters and evaluate\n",
        "    ann.set_parameters(best_parameters)\n",
        "    train_mae = evaluate_mae(ann, X_train, y_train)\n",
        "    test_mae = evaluate_mae(ann, X_test, y_test)\n",
        "\n",
        "    results = {\n",
        "        'train_mae': train_mae,\n",
        "        'test_mae': test_mae,\n",
        "        'best_fitness': best_fitness,\n",
        "        'convergence_history': pso.fitness_history,\n",
        "        'num_parameters': num_parameters\n",
        "    }\n",
        "\n",
        "    return results, ann, pso\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function - demonstrates basic usage\n",
        "    \"\"\"\n",
        "    print(\"=\"*80)\n",
        "    print(\" PSO-Optimized ANN for Concrete Compressive Strength Prediction\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    np.random.seed(42)\n",
        "    random.seed(42)\n",
        "\n",
        "    # Load and prepare data\n",
        "    print(\"\\n1. Loading and preparing data...\")\n",
        "    X_train, X_test, y_train, y_test = load_and_split_data('concrete_data.csv')\n",
        "\n",
        "    # Configure ANN architecture\n",
        "    print(\"\\n2. Configuring ANN architecture...\")\n",
        "    layer_sizes = [8, 10, 5, 1]  # 8 inputs, 2 hidden layers, 1 output\n",
        "    activation_functions = ['relu', 'relu', 'linear']\n",
        "\n",
        "    # Configure PSO parameters\n",
        "    print(\"\\n3. Configuring PSO parameters...\")\n",
        "    swarm_size = 30\n",
        "    max_iterations = 50\n",
        "    bounds = (-5.0, 5.0)\n",
        "\n",
        "    # Run experiment\n",
        "    print(\"\\n4. Running PSO-ANN optimization...\")\n",
        "    results, ann, pso = run_single_experiment(\n",
        "        X_train, X_test, y_train, y_test,\n",
        "        layer_sizes, activation_functions,\n",
        "        swarm_size, max_iterations, bounds,\n",
        "        random_seed=42\n",
        "    )\n",
        "\n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\" RESULTS\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Training MAE:   {results['train_mae']:.4f} MPa\")\n",
        "    print(f\"Test MAE:       {results['test_mae']:.4f} MPa\")\n",
        "    print(f\"Best Fitness:   {results['best_fitness']:.4f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Plot convergence\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(results['convergence_history'], linewidth=2)\n",
        "    plt.xlabel('Iteration', fontsize=12)\n",
        "    plt.ylabel('Best Fitness (MAE)', fontsize=12)\n",
        "    plt.title('PSO Convergence Curve', fontsize=14)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('pso_convergence.png', dpi=300)\n",
        "    print(\"\\nConvergence plot saved as 'pso_convergence.png'\")\n",
        "\n",
        "    return results, ann, pso\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "AWCDUF69Q6n2",
        "outputId": "22cdc660-7025-403b-b5f5-a9c86861690e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-3383845189.py, line 63)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3383845189.py\"\u001b[0;36m, line \u001b[0;32m63\u001b[0m\n\u001b[0;31m    data = pd.read_csv(/content/concrete_data.csv)\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    }
  ]
}
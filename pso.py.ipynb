{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "063P9WcR4lHS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "48ce86be-eade-4473-da40-505857c7a754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and preprocessing data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Concrete_Data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1976597931.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1976597931.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    659\u001b[0m     \"\"\"\n\u001b[1;32m    660\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading and preprocessing data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training samples: {X_train.shape[0]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1976597931.py\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \"\"\"\n\u001b[1;32m    327\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# Separate features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Concrete_Data.csv'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class ANN:\n",
        "    \"\"\"\n",
        "    Feedforward Artificial Neural Network with configurable architecture.\n",
        "    Implements multiple activation functions for regression tasks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_sizes, activation_functions):\n",
        "        \"\"\"\n",
        "        Initialize ANN with specified architecture.\n",
        "\n",
        "        Args:\n",
        "            layer_sizes: List of integers specifying neurons in each layer\n",
        "                        e.g., [8, 10, 5, 1] for 8 inputs, 2 hidden layers, 1 output\n",
        "            activation_functions: List of activation function names for each layer\n",
        "                                 e.g., ['relu', 'relu', 'linear']\n",
        "        \"\"\"\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.num_layers = len(layer_sizes)\n",
        "        self.activation_functions = activation_functions\n",
        "\n",
        "        # Initialize weights and biases randomly\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Create weight matrices between consecutive layers\n",
        "        for i in range(self.num_layers - 1):\n",
        "            # Xavier initialization for better convergence\n",
        "            limit = np.sqrt(6 / (layer_sizes[i] + layer_sizes[i+1]))\n",
        "            w = np.random.uniform(-limit, limit, (layer_sizes[i], layer_sizes[i+1]))\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        \"\"\"\n",
        "        Set network parameters (weights and biases) from a flat vector.\n",
        "        Used by PSO to update the network.\n",
        "\n",
        "        Args:\n",
        "            params: 1D numpy array containing all weights and biases\n",
        "        \"\"\"\n",
        "        idx = 0\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Reconstruct weight matrices and bias vectors from flat array\n",
        "        for i in range(self.num_layers - 1):\n",
        "            # Calculate size of weight matrix\n",
        "            w_size = self.layer_sizes[i] * self.layer_sizes[i+1]\n",
        "            # Extract and reshape weights\n",
        "            w = params[idx:idx+w_size].reshape(self.layer_sizes[i], self.layer_sizes[i+1])\n",
        "            idx += w_size\n",
        "\n",
        "            # Extract biases\n",
        "            b_size = self.layer_sizes[i+1]\n",
        "            b = params[idx:idx+b_size].reshape(1, self.layer_sizes[i+1])\n",
        "            idx += b_size\n",
        "\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def get_parameter_count(self):\n",
        "        \"\"\"\n",
        "        Calculate total number of parameters (weights + biases) in the network.\n",
        "\n",
        "        Returns:\n",
        "            Integer count of total parameters\n",
        "        \"\"\"\n",
        "        count = 0\n",
        "        for i in range(self.num_layers - 1):\n",
        "            # Weights: input_size * output_size\n",
        "            count += self.layer_sizes[i] * self.layer_sizes[i+1]\n",
        "            # Biases: output_size\n",
        "            count += self.layer_sizes[i+1]\n",
        "        return count\n",
        "\n",
        "    def activate(self, x, function_name):\n",
        "        \"\"\"\n",
        "        Apply activation function to input.\n",
        "\n",
        "        Args:\n",
        "            x: Input array\n",
        "            function_name: Name of activation function ('logistic', 'relu', 'tanh', 'linear')\n",
        "\n",
        "        Returns:\n",
        "            Activated output\n",
        "        \"\"\"\n",
        "        if function_name == 'logistic':\n",
        "            # Logistic sigmoid: 1 / (1 + e^(-x))\n",
        "            return 1 / (1 + np.exp(-np.clip(x, -500, 500)))  # Clip to prevent overflow\n",
        "        elif function_name == 'relu':\n",
        "            # ReLU: max(0, x)\n",
        "            return np.maximum(0, x)\n",
        "        elif function_name == 'tanh':\n",
        "            # Hyperbolic tangent\n",
        "            return np.tanh(x)\n",
        "        elif function_name == 'linear':\n",
        "            # Linear (no activation)\n",
        "            return x\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown activation function: {function_name}\")\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Perform forward propagation through the network.\n",
        "\n",
        "        Args:\n",
        "            X: Input data, shape (n_samples, n_features)\n",
        "\n",
        "        Returns:\n",
        "            Network output, shape (n_samples, n_outputs)\n",
        "        \"\"\"\n",
        "        activation = X\n",
        "\n",
        "        # Propagate through each layer\n",
        "        for i in range(len(self.weights)):\n",
        "            # Linear transformation: z = activation * weights + bias\n",
        "            z = np.dot(activation, self.weights[i]) + self.biases[i]\n",
        "            # Apply activation function\n",
        "            activation = self.activate(z, self.activation_functions[i])\n",
        "\n",
        "        return activation\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions on input data.\n",
        "\n",
        "        Args:\n",
        "            X: Input data\n",
        "\n",
        "        Returns:\n",
        "            Predictions\n",
        "        \"\"\"\n",
        "        return self.forward(X)\n",
        "        import numpy as np\n",
        "\n",
        "class PSO:\n",
        "    \"\"\"\n",
        "    Particle Swarm Optimization implementation following Algorithm 39\n",
        "    from \"Essentials of Metaheuristics\" by Sean Luke.\n",
        "    Uses informants topology rather than global best.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fitness_function, dimensions, bounds,\n",
        "                 swarm_size=30, max_iterations=100,\n",
        "                 num_informants=3, w=0.729, c1=1.49445, c2=1.49445):\n",
        "        \"\"\"\n",
        "        Initialize PSO with specified parameters.\n",
        "\n",
        "        Args:\n",
        "            fitness_function: Function to minimize (lower is better)\n",
        "            dimensions: Number of dimensions in solution space\n",
        "            bounds: Tuple of (min, max) for parameter values\n",
        "            swarm_size: Number of particles in swarm\n",
        "            max_iterations: Maximum number of iterations\n",
        "            num_informants: Number of informants per particle\n",
        "            w: Inertia weight\n",
        "            c1: Cognitive coefficient (personal best influence)\n",
        "            c2: Social coefficient (neighborhood best influence)\n",
        "        \"\"\"\n",
        "        self.fitness_function = fitness_function\n",
        "        self.dimensions = dimensions\n",
        "        self.bounds = bounds\n",
        "        self.swarm_size = swarm_size\n",
        "        self.max_iterations = max_iterations\n",
        "        self.num_informants = num_informants\n",
        "        self.w = w  # Inertia weight\n",
        "        self.c1 = c1  # Cognitive coefficient\n",
        "        self.c2 = c2  # Social coefficient\n",
        "\n",
        "        # For tracking best solution found\n",
        "        self.global_best_position = None\n",
        "        self.global_best_fitness = float('inf')\n",
        "        self.fitness_history = []\n",
        "\n",
        "    def initialize_swarm(self):\n",
        "        \"\"\"\n",
        "        Initialize particle positions and velocities.\n",
        "        Corresponds to lines 1-2 of Algorithm 39.\n",
        "\n",
        "        Returns:\n",
        "            positions: Random initial positions\n",
        "            velocities: Random initial velocities\n",
        "        \"\"\"\n",
        "        # Line 1: Initialize positions uniformly in search space\n",
        "        positions = np.random.uniform(\n",
        "            self.bounds[0],\n",
        "            self.bounds[1],\n",
        "            (self.swarm_size, self.dimensions)\n",
        "        )\n",
        "\n",
        "        # Line 2: Initialize velocities\n",
        "        velocity_range = (self.bounds[1] - self.bounds[0]) * 0.1\n",
        "        velocities = np.random.uniform(\n",
        "            -velocity_range,\n",
        "            velocity_range,\n",
        "            (self.swarm_size, self.dimensions)\n",
        "        )\n",
        "\n",
        "        return positions, velocities\n",
        "\n",
        "    def create_informants(self):\n",
        "        \"\"\"\n",
        "        Create informant topology (ring topology).\n",
        "        Each particle is influenced by k neighbors.\n",
        "        Corresponds to line 3 of Algorithm 39.\n",
        "\n",
        "        Returns:\n",
        "            List of lists, where informants[i] contains indices of particle i's informants\n",
        "        \"\"\"\n",
        "        # Line 3: Assign informants to particles\n",
        "        informants = []\n",
        "        for i in range(self.swarm_size):\n",
        "            # Ring topology: include neighbors on both sides\n",
        "            neighbors = []\n",
        "            for j in range(1, self.num_informants + 1):\n",
        "                left = (i - j) % self.swarm_size\n",
        "                right = (i + j) % self.swarm_size\n",
        "                neighbors.append(left)\n",
        "                if len(neighbors) < self.num_informants:\n",
        "                    neighbors.append(right)\n",
        "            # Include self and limit to num_informants\n",
        "            neighbors = list(set([i] + neighbors[:self.num_informants-1]))\n",
        "            informants.append(neighbors)\n",
        "        return informants\n",
        "\n",
        "    def optimize(self, verbose=True):\n",
        "        \"\"\"\n",
        "        Main PSO optimization loop.\n",
        "        Follows Algorithm 39 from Essentials of Metaheuristics.\n",
        "\n",
        "        Returns:\n",
        "            best_position: Best solution found\n",
        "            best_fitness: Fitness of best solution\n",
        "        \"\"\"\n",
        "        # Lines 1-3: Initialize swarm\n",
        "        positions, velocities = self.initialize_swarm()\n",
        "        informants = self.create_informants()\n",
        "\n",
        "        # Line 4: Initialize personal bests\n",
        "        personal_best_positions = positions.copy()\n",
        "        personal_best_fitness = np.array([self.fitness_function(p) for p in positions])\n",
        "\n",
        "        # Initialize neighborhood bests\n",
        "        neighborhood_best_positions = np.zeros_like(positions)\n",
        "        neighborhood_best_fitness = np.full(self.swarm_size, float('inf'))\n",
        "\n",
        "        # Track global best\n",
        "        best_idx = np.argmin(personal_best_fitness)\n",
        "        self.global_best_position = personal_best_positions[best_idx].copy()\n",
        "        self.global_best_fitness = personal_best_fitness[best_idx]\n",
        "\n",
        "        # Line 5: Main loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "\n",
        "            # Update neighborhood bests for each particle\n",
        "            for i in range(self.swarm_size):\n",
        "                # Find best among informants\n",
        "                informant_fitness = [personal_best_fitness[j] for j in informants[i]]\n",
        "                best_informant_idx = informants[i][np.argmin(informant_fitness)]\n",
        "                neighborhood_best_positions[i] = personal_best_positions[best_informant_idx]\n",
        "                neighborhood_best_fitness[i] = personal_best_fitness[best_informant_idx]\n",
        "\n",
        "            # Update each particle\n",
        "            for i in range(self.swarm_size):\n",
        "\n",
        "                # Line 6: Generate random values\n",
        "                rp = np.random.uniform(0, 1, self.dimensions)\n",
        "                rg = np.random.uniform(0, 1, self.dimensions)\n",
        "\n",
        "                # Line 7: Update velocity\n",
        "                # v_i = w * v_i + c1 * rp * (p_i - x_i) + c2 * rg * (n_i - x_i)\n",
        "                cognitive = self.c1 * rp * (personal_best_positions[i] - positions[i])\n",
        "                social = self.c2 * rg * (neighborhood_best_positions[i] - positions[i])\n",
        "                velocities[i] = self.w * velocities[i] + cognitive + social\n",
        "\n",
        "                # Line 8: Update position\n",
        "                positions[i] = positions[i] + velocities[i]\n",
        "\n",
        "                # Line 9: Enforce bounds (clamping strategy)\n",
        "                positions[i] = np.clip(positions[i], self.bounds[0], self.bounds[1])\n",
        "\n",
        "                # Line 10: Evaluate fitness\n",
        "                fitness = self.fitness_function(positions[i])\n",
        "\n",
        "                # Line 11: Update personal best\n",
        "                if fitness < personal_best_fitness[i]:\n",
        "                    personal_best_positions[i] = positions[i].copy()\n",
        "                    personal_best_fitness[i] = fitness\n",
        "\n",
        "                    # Update global best if necessary\n",
        "                    if fitness < self.global_best_fitness:\n",
        "                        self.global_best_position = positions[i].copy()\n",
        "                        self.global_best_fitness = fitness\n",
        "\n",
        "            # Store best fitness for this iteration\n",
        "            self.fitness_history.append(self.global_best_fitness)\n",
        "\n",
        "            if verbose and (iteration + 1) % 10 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.max_iterations}, \"\n",
        "                      f\"Best Fitness: {self.global_best_fitness:.4f}\")\n",
        "\n",
        "        return self.global_best_position, self.global_best_fitness\n",
        "        import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "# Import the ANN and PSO classes (assuming they're in separate files)\n",
        "# from ann import ANN\n",
        "# from pso import PSO\n",
        "\n",
        "def load_and_preprocess_data(filepath='Concrete_Data.csv'):\n",
        "    \"\"\"\n",
        "    Load and preprocess the concrete strength dataset.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to CSV file\n",
        "\n",
        "    Returns:\n",
        "        X_train, X_test, y_train, y_test, scaler_X, scaler_y\n",
        "    \"\"\"\n",
        "    # Load data\n",
        "    data = pd.read_csv(filepath)\n",
        "\n",
        "    # Separate features and target\n",
        "    X = data.iloc[:, :-1].values  # First 8 columns are features\n",
        "    y = data.iloc[:, -1].values.reshape(-1, 1)  # Last column is target\n",
        "\n",
        "    # Split into training (70%) and testing (30%)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.3, random_state=42\n",
        "    )\n",
        "\n",
        "    # Normalize features and target for better ANN performance\n",
        "    scaler_X = StandardScaler()\n",
        "    scaler_y = StandardScaler()\n",
        "\n",
        "    X_train = scaler_X.fit_transform(X_train)\n",
        "    X_test = scaler_X.transform(X_test)\n",
        "    y_train = scaler_y.fit_transform(y_train)\n",
        "    y_test = scaler_y.transform(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, scaler_X, scaler_y\n",
        "\n",
        "\n",
        "def mean_absolute_error(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate Mean Absolute Error.\n",
        "\n",
        "    Args:\n",
        "        y_true: True values\n",
        "        y_pred: Predicted values\n",
        "\n",
        "    Returns:\n",
        "        MAE value\n",
        "    \"\"\"\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "\n",
        "def create_fitness_function(ann, X_train, y_train):\n",
        "    \"\"\"\n",
        "    Create fitness function for PSO that evaluates ANN performance.\n",
        "    Couples PSO and ANN together.\n",
        "\n",
        "    Args:\n",
        "        ann: ANN instance\n",
        "        X_train: Training features\n",
        "        y_train: Training targets\n",
        "\n",
        "    Returns:\n",
        "        Fitness function that takes parameter vector and returns MAE\n",
        "    \"\"\"\n",
        "    def fitness(params):\n",
        "        \"\"\"\n",
        "        Fitness function evaluates how well the ANN performs with given parameters.\n",
        "        Lower MAE = better fitness.\n",
        "\n",
        "        Args:\n",
        "            params: Flat vector of ANN weights and biases\n",
        "\n",
        "        Returns:\n",
        "            Mean Absolute Error on training set\n",
        "        \"\"\"\n",
        "        # Set ANN parameters from PSO particle\n",
        "        ann.set_parameters(params)\n",
        "\n",
        "        # Get predictions\n",
        "        predictions = ann.predict(X_train)\n",
        "\n",
        "        # Calculate and return error (fitness to minimize)\n",
        "        mae = mean_absolute_error(y_train, predictions)\n",
        "        return mae\n",
        "\n",
        "    return fitness\n",
        "\n",
        "\n",
        "def evaluate_ann(ann, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate trained ANN on test set.\n",
        "\n",
        "    Args:\n",
        "        ann: Trained ANN\n",
        "        X_test: Test features\n",
        "        y_test: Test targets\n",
        "\n",
        "    Returns:\n",
        "        Test MAE\n",
        "    \"\"\"\n",
        "    predictions = ann.predict(X_test)\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    return mae\n",
        "\n",
        "\n",
        "def run_single_experiment(layer_sizes, activations, swarm_size, max_iterations,\n",
        "                         w, c1, c2, num_informants, X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Run a single PSO-ANN experiment.\n",
        "\n",
        "    Args:\n",
        "        layer_sizes: ANN architecture\n",
        "        activations: Activation functions for each layer\n",
        "        swarm_size: PSO swarm size\n",
        "        max_iterations: PSO iterations\n",
        "        w, c1, c2: PSO coefficients\n",
        "        num_informants: Number of informants\n",
        "        X_train, y_train: Training data\n",
        "        X_test, y_test: Test data\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results\n",
        "    \"\"\"\n",
        "    # Create ANN\n",
        "    ann = ANN(layer_sizes, activations)\n",
        "\n",
        "    # Get parameter count and bounds\n",
        "    param_count = ann.get_parameter_count()\n",
        "    bounds = (-2, 2)  # Typical range for neural network weights\n",
        "\n",
        "    # Create fitness function\n",
        "    fitness_func = create_fitness_function(ann, X_train, y_train)\n",
        "\n",
        "    # Initialize PSO\n",
        "    pso = PSO(\n",
        "        fitness_function=fitness_func,\n",
        "        dimensions=param_count,\n",
        "        bounds=bounds,\n",
        "        swarm_size=swarm_size,\n",
        "        max_iterations=max_iterations,\n",
        "        num_informants=num_informants,\n",
        "        w=w,\n",
        "        c1=c1,\n",
        "        c2=c2\n",
        "    )\n",
        "\n",
        "    # Run optimization\n",
        "    best_params, train_mae = pso.optimize(verbose=False)\n",
        "\n",
        "    # Set best parameters and evaluate on test set\n",
        "    ann.set_parameters(best_params)\n",
        "    test_mae = evaluate_ann(ann, X_test, y_test)\n",
        "\n",
        "    return {\n",
        "        'train_mae': train_mae,\n",
        "        'test_mae': test_mae,\n",
        "        'fitness_history': pso.fitness_history\n",
        "    }\n",
        "\n",
        "\n",
        "def experiment_architecture(X_train, y_train, X_test, y_test, num_runs=10):\n",
        "    \"\"\"\n",
        "    Experiment 1: Test different ANN architectures.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXPERIMENT 1: ANN Architecture Investigation\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Different architectures to test\n",
        "    architectures = [\n",
        "        ([8, 5, 1], ['relu', 'linear'], '8-5-1 (ReLU)'),\n",
        "        ([8, 10, 1], ['relu', 'linear'], '8-10-1 (ReLU)'),\n",
        "        ([8, 20, 1], ['relu', 'linear'], '8-20-1 (ReLU)'),\n",
        "        ([8, 10, 5, 1], ['relu', 'relu', 'linear'], '8-10-5-1 (ReLU-ReLU)'),\n",
        "        ([8, 10, 1], ['tanh', 'linear'], '8-10-1 (Tanh)'),\n",
        "        ([8, 10, 1], ['logistic', 'linear'], '8-10-1 (Logistic)'),\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for layers, activations, name in architectures:\n",
        "        print(f\"\\nTesting architecture: {name}\")\n",
        "        train_maes = []\n",
        "        test_maes = []\n",
        "\n",
        "        for run in range(num_runs):\n",
        "            result = run_single_experiment(\n",
        "                layer_sizes=layers,\n",
        "                activations=activations,\n",
        "                swarm_size=30,\n",
        "                max_iterations=50,\n",
        "                w=0.729,\n",
        "                c1=1.49445,\n",
        "                c2=1.49445,\n",
        "                num_informants=3,\n",
        "                X_train=X_train,\n",
        "                y_train=y_train,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test\n",
        "            )\n",
        "            train_maes.append(result['train_mae'])\n",
        "            test_maes.append(result['test_mae'])\n",
        "            print(f\"  Run {run+1}/{num_runs}: Test MAE = {result['test_mae']:.4f}\")\n",
        "\n",
        "        results[name] = {\n",
        "            'train_mae_mean': np.mean(train_maes),\n",
        "            'train_mae_std': np.std(train_maes),\n",
        "            'test_mae_mean': np.mean(test_maes),\n",
        "            'test_mae_std': np.std(test_maes)\n",
        "        }\n",
        "\n",
        "        print(f\"  Average Test MAE: {results[name]['test_mae_mean']:.4f} ± {results[name]['test_mae_std']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def experiment_swarm_allocation(X_train, y_train, X_test, y_test, num_runs=10):\n",
        "    \"\"\"\n",
        "    Experiment 2: Test different swarm size vs iterations allocations.\n",
        "    Fixed budget of 500 evaluations.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXPERIMENT 2: Swarm Size vs Iterations (Budget=500)\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Different allocations with budget of 500\n",
        "    allocations = [\n",
        "        (10, 50, '10x50'),\n",
        "        (25, 20, '25x20'),\n",
        "        (50, 10, '50x10'),\n",
        "        (100, 5, '100x5'),\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for swarm_size, iterations, name in allocations:\n",
        "        print(f\"\\nTesting allocation: {name} (swarm={swarm_size}, iter={iterations})\")\n",
        "        train_maes = []\n",
        "        test_maes = []\n",
        "\n",
        "        for run in range(num_runs):\n",
        "            result = run_single_experiment(\n",
        "                layer_sizes=[8, 10, 1],\n",
        "                activations=['relu', 'linear'],\n",
        "                swarm_size=swarm_size,\n",
        "                max_iterations=iterations,\n",
        "                w=0.729,\n",
        "                c1=1.49445,\n",
        "                c2=1.49445,\n",
        "                num_informants=3,\n",
        "                X_train=X_train,\n",
        "                y_train=y_train,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test\n",
        "            )\n",
        "            train_maes.append(result['train_mae'])\n",
        "            test_maes.append(result['test_mae'])\n",
        "            print(f\"  Run {run+1}/{num_runs}: Test MAE = {result['test_mae']:.4f}\")\n",
        "\n",
        "        results[name] = {\n",
        "            'train_mae_mean': np.mean(train_maes),\n",
        "            'train_mae_std': np.std(train_maes),\n",
        "            'test_mae_mean': np.mean(test_maes),\n",
        "            'test_mae_std': np.std(test_maes)\n",
        "        }\n",
        "\n",
        "        print(f\"  Average Test MAE: {results[name]['test_mae_mean']:.4f} ± {results[name]['test_mae_std']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def experiment_coefficients(X_train, y_train, X_test, y_test, num_runs=10):\n",
        "    \"\"\"\n",
        "    Experiment 3: Test different PSO acceleration coefficients.\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXPERIMENT 3: PSO Acceleration Coefficients\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Different coefficient combinations\n",
        "    coefficient_sets = [\n",
        "        (0.729, 1.49445, 1.49445, 'Standard (w=0.729, c1=c2=1.49)'),\n",
        "        (0.5, 2.0, 2.0, 'High acceleration (w=0.5, c1=c2=2.0)'),\n",
        "        (0.9, 2.0, 0.5, 'Cognitive-heavy (w=0.9, c1=2.0, c2=0.5)'),\n",
        "        (0.9, 0.5, 2.0, 'Social-heavy (w=0.9, c1=0.5, c2=2.0)'),\n",
        "        (0.4, 1.0, 1.0, 'Low inertia (w=0.4, c1=c2=1.0)'),\n",
        "    ]\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    for w, c1, c2, name in coefficient_sets:\n",
        "        print(f\"\\nTesting: {name}\")\n",
        "        train_maes = []\n",
        "        test_maes = []\n",
        "\n",
        "        for run in range(num_runs):\n",
        "            result = run_single_experiment(\n",
        "                layer_sizes=[8, 10, 1],\n",
        "                activations=['relu', 'linear'],\n",
        "                swarm_size=30,\n",
        "                max_iterations=50,\n",
        "                w=w,\n",
        "                c1=c1,\n",
        "                c2=c2,\n",
        "                num_informants=3,\n",
        "                X_train=X_train,\n",
        "                y_train=y_train,\n",
        "                X_test=X_test,\n",
        "                y_test=y_test\n",
        "            )\n",
        "            train_maes.append(result['train_mae'])\n",
        "            test_maes.append(result['test_mae'])\n",
        "            print(f\"  Run {run+1}/{num_runs}: Test MAE = {result['test_mae']:.4f}\")\n",
        "\n",
        "        results[name] = {\n",
        "            'train_mae_mean': np.mean(train_maes),\n",
        "            'train_mae_std': np.std(train_maes),\n",
        "            'test_mae_mean': np.mean(test_maes),\n",
        "            'test_mae_std': np.std(test_maes)\n",
        "        }\n",
        "\n",
        "        print(f\"  Average Test MAE: {results[name]['test_mae_mean']:.4f} ± {results[name]['test_mae_std']:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_results(results, filename):\n",
        "    \"\"\"Save results to JSON file.\"\"\"\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(results, f, indent=2)\n",
        "    print(f\"\\nResults saved to {filename}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function.\n",
        "    \"\"\"\n",
        "    print(\"Loading and preprocessing data...\")\n",
        "    X_train, X_test, y_train, y_test, scaler_X, scaler_y = load_and_preprocess_data()\n",
        "\n",
        "    print(f\"Training samples: {X_train.shape[0]}\")\n",
        "    print(f\"Testing samples: {X_test.shape[0]}\")\n",
        "    print(f\"Features: {X_train.shape[1]}\")\n",
        "\n",
        "    # Run experiments\n",
        "    results = {}\n",
        "\n",
        "    # Experiment 1: Architecture\n",
        "    results['architecture'] = experiment_architecture(X_train, y_train, X_test, y_test, num_runs=10)\n",
        "\n",
        "    # Experiment 2: Swarm allocation\n",
        "    results['swarm_allocation'] = experiment_swarm_allocation(X_train, y_train, X_test, y_test, num_runs=10)\n",
        "\n",
        "    # Experiment 3: Coefficients\n",
        "    results['coefficients'] = experiment_coefficients(X_train, y_train, X_test, y_test, num_runs=10)\n",
        "\n",
        "    # Save all results\n",
        "    save_results(results, 'experiment_results.json')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ALL EXPERIMENTS COMPLETED\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "    import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "def plot_convergence(fitness_history, title=\"PSO Convergence\"):\n",
        "    \"\"\"\n",
        "    Plot PSO convergence curve.\n",
        "\n",
        "    Args:\n",
        "        fitness_history: List of best fitness values per iteration\n",
        "        title: Plot title\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(fitness_history, linewidth=2)\n",
        "    plt.xlabel('Iteration', fontsize=12)\n",
        "    plt.ylabel('Best Fitness (MAE)', fontsize=12)\n",
        "    plt.title(title, fontsize=14, fontweight='bold')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{title.replace(\" \", \"_\")}.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_architecture_comparison(results):\n",
        "    \"\"\"\n",
        "    Plot comparison of different ANN architectures.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary with architecture experiment results\n",
        "    \"\"\"\n",
        "    names = list(results.keys())\n",
        "    test_means = [results[name]['test_mae_mean'] for name in names]\n",
        "    test_stds = [results[name]['test_mae_std'] for name in names]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(len(names))\n",
        "    plt.bar(x, test_means, yerr=test_stds, capsize=5, alpha=0.7, color='steelblue')\n",
        "    plt.xlabel('Architecture', fontsize=12)\n",
        "    plt.ylabel('Test MAE', fontsize=12)\n",
        "    plt.title('ANN Architecture Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(x, names, rotation=45, ha='right')\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('architecture_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_swarm_allocation(results):\n",
        "    \"\"\"\n",
        "    Plot comparison of swarm size vs iterations allocations.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary with swarm allocation experiment results\n",
        "    \"\"\"\n",
        "    names = list(results.keys())\n",
        "    test_means = [results[name]['test_mae_mean'] for name in names]\n",
        "    test_stds = [results[name]['test_mae_std'] for name in names]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    x = np.arange(len(names))\n",
        "    plt.bar(x, test_means, yerr=test_stds, capsize=5, alpha=0.7, color='coral')\n",
        "    plt.xlabel('Allocation (Swarm Size × Iterations)', fontsize=12)\n",
        "    plt.ylabel('Test MAE', fontsize=12)\n",
        "    plt.title('Swarm Allocation Comparison (Budget=500)', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(x, names)\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('swarm_allocation_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_coefficient_comparison(results):\n",
        "    \"\"\"\n",
        "    Plot comparison of different PSO coefficient settings.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary with coefficient experiment results\n",
        "    \"\"\"\n",
        "    names = list(results.keys())\n",
        "    test_means = [results[name]['test_mae_mean'] for name in names]\n",
        "    test_stds = [results[name]['test_mae_std'] for name in names]\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(len(names))\n",
        "    plt.bar(x, test_means, yerr=test_stds, capsize=5, alpha=0.7, color='mediumseagreen')\n",
        "    plt.xlabel('Coefficient Setting', fontsize=12)\n",
        "    plt.ylabel('Test MAE', fontsize=12)\n",
        "    plt.title('PSO Coefficient Comparison', fontsize=14, fontweight='bold')\n",
        "    plt.xticks(x, names, rotation=45, ha='right')\n",
        "    plt.grid(True, alpha=0.3, axis='y')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('coefficient_comparison.png', dpi=300)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def create_results_table(results):\n",
        "    \"\"\"\n",
        "    Create formatted results table for report.\n",
        "\n",
        "    Args:\n",
        "        results: Dictionary with experiment results\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"RESULTS SUMMARY TABLE\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for experiment_name, experiment_results in results.items():\n",
        "        print(f\"\\n{experiment_name.upper().replace('_', ' ')}\")\n",
        "        print(\"-\" * 80)\n",
        "        print(f\"{'Configuration':<40} {'Train MAE':<20} {'Test MAE':<20}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for config_name, config_results in experiment_results.items():\n",
        "            train_str = f\"{config_results['train_mae_mean']:.4f} ± {config_results['train_mae_std']:.4f}\"\n",
        "            test_str = f\"{config_results['test_mae_mean']:.4f} ± {config_results['test_mae_std']:.4f}\"\n",
        "            print(f\"{config_name:<40} {train_str:<20} {test_str:<20}\")\n",
        "        print()\n",
        "\n",
        "\n",
        "def analyze_results(results_file='experiment_results.json'):\n",
        "    \"\"\"\n",
        "    Load and analyze experimental results.\n",
        "\n",
        "    Args:\n",
        "        results_file: Path to JSON results file\n",
        "    \"\"\"\n",
        "    with open(results_file, 'r') as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    # Print results table\n",
        "    create_results_table(results)\n",
        "\n",
        "    # Create visualizations\n",
        "    plot_architecture_comparison(results['architecture'])\n",
        "    plot_swarm_allocation(results['swarm_allocation'])\n",
        "    plot_coefficient_comparison(results['coefficients'])\n",
        "\n",
        "    # Find best configurations\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"BEST CONFIGURATIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    for experiment_name, experiment_results in results.items():\n",
        "        best_config = min(experiment_results.items(),\n",
        "                         key=lambda x: x[1]['test_mae_mean'])\n",
        "        print(f\"\\n{experiment_name.upper().replace('_', ' ')}:\")\n",
        "        print(f\"  Best: {best_config[0]}\")\n",
        "        print(f\"  Test MAE: {best_config[1]['test_mae_mean']:.4f} ± {best_config[1]['test_mae_std']:.4f}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Analyze results if they exist\n",
        "    try:\n",
        "        analyze_results()\n",
        "    except FileNotFoundError:\n",
        "        print(\"No results file found. Run main.py first to generate results.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class PSO:\n",
        "    \"\"\"\n",
        "    Particle Swarm Optimization implementation following Algorithm 39\n",
        "    from \"Essentials of Metaheuristics\" by Sean Luke.\n",
        "    Uses informants topology rather than global best.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fitness_function, dimensions, bounds,\n",
        "                 swarm_size=30, max_iterations=100,\n",
        "                 num_informants=3, w=0.729, c1=1.49445, c2=1.49445):\n",
        "        \"\"\"\n",
        "        Initialize PSO with specified parameters.\n",
        "\n",
        "        Args:\n",
        "            fitness_function: Function to minimize (lower is better)\n",
        "            dimensions: Number of dimensions in solution space\n",
        "            bounds: Tuple of (min, max) for parameter values\n",
        "            swarm_size: Number of particles in swarm\n",
        "            max_iterations: Maximum number of iterations\n",
        "            num_informants: Number of informants per particle\n",
        "            w: Inertia weight\n",
        "            c1: Cognitive coefficient (personal best influence)\n",
        "            c2: Social coefficient (neighborhood best influence)\n",
        "        \"\"\"\n",
        "        self.fitness_function = fitness_function\n",
        "        self.dimensions = dimensions\n",
        "        self.bounds = bounds\n",
        "        self.swarm_size = swarm_size\n",
        "        self.max_iterations = max_iterations\n",
        "        self.num_informants = num_informants\n",
        "        self.w = w  # Inertia weight\n",
        "        self.c1 = c1  # Cognitive coefficient\n",
        "        self.c2 = c2  # Social coefficient\n",
        "\n",
        "        # For tracking best solution found\n",
        "        self.global_best_position = None\n",
        "        self.global_best_fitness = float('inf')\n",
        "        self.fitness_history = []\n",
        "\n",
        "    def initialize_swarm(self):\n",
        "        \"\"\"\n",
        "        Initialize particle positions and velocities.\n",
        "        Corresponds to lines 1-2 of Algorithm 39.\n",
        "\n",
        "        Returns:\n",
        "            positions: Random initial positions\n",
        "            velocities: Random initial velocities\n",
        "        \"\"\"\n",
        "        # Line 1: Initialize positions uniformly in search space\n",
        "        positions = np.random.uniform(\n",
        "            self.bounds[0],\n",
        "            self.bounds[1],\n",
        "            (self.swarm_size, self.dimensions)\n",
        "        )\n",
        "\n",
        "        # Line 2: Initialize velocities\n",
        "        velocity_range = (self.bounds[1] - self.bounds[0]) * 0.1\n",
        "        velocities = np.random.uniform(\n",
        "            -velocity_range,\n",
        "            velocity_range,\n",
        "            (self.swarm_size, self.dimensions)\n",
        "        )\n",
        "\n",
        "        return positions, velocities\n",
        "\n",
        "    def create_informants(self):\n",
        "        \"\"\"\n",
        "        Create informant topology (ring topology).\n",
        "        Each particle is influenced by k neighbors.\n",
        "        Corresponds to line 3 of Algorithm 39.\n",
        "\n",
        "        Returns:\n",
        "            List of lists, where informants[i] contains indices of particle i's informants\n",
        "        \"\"\"\n",
        "        # Line 3: Assign informants to particles\n",
        "        informants = []\n",
        "        for i in range(self.swarm_size):\n",
        "            # Ring topology: include neighbors on both sides\n",
        "            neighbors = []\n",
        "            for j in range(1, self.num_informants + 1):\n",
        "                left = (i - j) % self.swarm_size\n",
        "                right = (i + j) % self.swarm_size\n",
        "                neighbors.append(left)\n",
        "                if len(neighbors) < self.num_informants:\n",
        "                    neighbors.append(right)\n",
        "            # Include self and limit to num_informants\n",
        "            neighbors = list(set([i] + neighbors[:self.num_informants-1]))\n",
        "            informants.append(neighbors)\n",
        "        return informants\n",
        "\n",
        "    def optimize(self, verbose=True):\n",
        "        \"\"\"\n",
        "        Main PSO optimization loop.\n",
        "        Follows Algorithm 39 from Essentials of Metaheuristics.\n",
        "\n",
        "        Returns:\n",
        "            best_position: Best solution found\n",
        "            best_fitness: Fitness of best solution\n",
        "        \"\"\"\n",
        "        # Lines 1-3: Initialize swarm\n",
        "        positions, velocities = self.initialize_swarm()\n",
        "        informants = self.create_informants()\n",
        "\n",
        "        # Line 4: Initialize personal bests\n",
        "        personal_best_positions = positions.copy()\n",
        "        personal_best_fitness = np.array([self.fitness_function(p) for p in positions])\n",
        "\n",
        "        # Initialize neighborhood bests\n",
        "        neighborhood_best_positions = np.zeros_like(positions)\n",
        "        neighborhood_best_fitness = np.full(self.swarm_size, float('inf'))\n",
        "\n",
        "        # Track global best\n",
        "        best_idx = np.argmin(personal_best_fitness)\n",
        "        self.global_best_position = personal_best_positions[best_idx].copy()\n",
        "        self.global_best_fitness = personal_best_fitness[best_idx]\n",
        "\n",
        "        # Line 5: Main loop\n",
        "        for iteration in range(self.max_iterations):\n",
        "\n",
        "            # Update neighborhood bests for each particle\n",
        "            for i in range(self.swarm_size):\n",
        "                # Find best among informants\n",
        "                informant_fitness = [personal_best_fitness[j] for j in informants[i]]\n",
        "                best_informant_idx = informants[i][np.argmin(informant_fitness)]\n",
        "                neighborhood_best_positions[i] = personal_best_positions[best_informant_idx]\n",
        "                neighborhood_best_fitness[i] = personal_best_fitness[best_informant_idx]\n",
        "\n",
        "            # Update each particle\n",
        "            for i in range(self.swarm_size):\n",
        "\n",
        "                # Line 6: Generate random values\n",
        "                rp = np.random.uniform(0, 1, self.dimensions)\n",
        "                rg = np.random.uniform(0, 1, self.dimensions)\n",
        "\n",
        "                # Line 7: Update velocity\n",
        "                # v_i = w * v_i + c1 * rp * (p_i - x_i) + c2 * rg * (n_i - x_i)\n",
        "                cognitive = self.c1 * rp * (personal_best_positions[i] - positions[i])\n",
        "                social = self.c2 * rg * (neighborhood_best_positions[i] - positions[i])\n",
        "                velocities[i] = self.w * velocities[i] + cognitive + social\n",
        "\n",
        "                # Line 8: Update position\n",
        "                positions[i] = positions[i] + velocities[i]\n",
        "\n",
        "                # Line 9: Enforce bounds (clamping strategy)\n",
        "                positions[i] = np.clip(positions[i], self.bounds[0], self.bounds[1])\n",
        "\n",
        "                # Line 10: Evaluate fitness\n",
        "                fitness = self.fitness_function(positions[i])\n",
        "\n",
        "                # Line 11: Update personal best\n",
        "                if fitness < personal_best_fitness[i]:\n",
        "                    personal_best_positions[i] = positions[i].copy()\n",
        "                    personal_best_fitness[i] = fitness\n",
        "\n",
        "                    # Update global best if necessary\n",
        "                    if fitness < self.global_best_fitness:\n",
        "                        self.global_best_position = positions[i].copy()\n",
        "                        self.global_best_fitness = fitness\n",
        "\n",
        "            # Store best fitness for this iteration\n",
        "            self.fitness_history.append(self.global_best_fitness)\n",
        "\n",
        "            if verbose and (iteration + 1) % 10 == 0:\n",
        "                print(f\"Iteration {iteration + 1}/{self.max_iterations}, \"\n",
        "                      f\"Best Fitness: {self.global_best_fitness:.4f}\")\n",
        "\n",
        "        return self.global_best_position, self.global_best_fitness"
      ],
      "metadata": {
        "id": "Db7Xya-6KcCW"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}
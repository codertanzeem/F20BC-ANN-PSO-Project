{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6dPooaXNPpnJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from typing import List, Callable, Tuple, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# ACTIVATION FUNCTIONS\n",
        "# ============================================================================\n",
        "\n",
        "def logistic(x):\n",
        "    \"\"\"\n",
        "    Logistic activation function: 1 / (1 + e^-x)\n",
        "    Clips input to prevent overflow in exponential calculation\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "def relu(x):\n",
        "    \"\"\"\n",
        "    ReLU (Rectified Linear Unit) activation function: max(0, x)\n",
        "    Returns x if x > 0, otherwise returns 0\n",
        "    \"\"\"\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def tanh(x):\n",
        "    \"\"\"\n",
        "    Hyperbolic tangent activation function: tanh(x)\n",
        "    Output range: (-1, 1)\n",
        "    \"\"\"\n",
        "    return np.tanh(x)\n",
        "\n",
        "def linear(x):\n",
        "    \"\"\"\n",
        "    Linear activation function (identity): f(x) = x\n",
        "    Typically used in output layer for regression tasks\n",
        "    \"\"\"\n",
        "    return x\n",
        "\n",
        "# Dictionary for easy activation function lookup\n",
        "ACTIVATION_FUNCTIONS = {\n",
        "    'logistic': logistic,\n",
        "    'relu': relu,\n",
        "    'tanh': tanh,\n",
        "    'linear': linear\n",
        "}\n",
        "\n",
        "# ============================================================================\n",
        "# ARTIFICIAL NEURAL NETWORK (ANN) CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class ANN:\n",
        "    \"\"\"\n",
        "    Multi-layer Feedforward Artificial Neural Network\n",
        "\n",
        "    This implementation allows configurable:\n",
        "    - Number of layers\n",
        "    - Number of neurons per layer\n",
        "    - Activation function per layer\n",
        "\n",
        "    The network is trained using PSO (not backpropagation)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer_sizes: List[int], activation_functions: List[str]):\n",
        "        \"\"\"\n",
        "        Initialize the ANN architecture\n",
        "\n",
        "        Args:\n",
        "            layer_sizes: List of integers, e.g., [8, 10, 5, 1]\n",
        "                        Specifies number of neurons in each layer\n",
        "                        First element = input layer, last = output layer\n",
        "            activation_functions: List of activation function names\n",
        "                                 One for each layer AFTER input layer\n",
        "                                 e.g., ['relu', 'relu', 'linear'] for 3 hidden/output layers\n",
        "        \"\"\"\n",
        "        # Store architecture configuration\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.num_layers = len(layer_sizes)\n",
        "\n",
        "        # Validate activation functions list\n",
        "        if len(activation_functions) != self.num_layers - 1:\n",
        "            raise ValueError(f\"Need {self.num_layers - 1} activation functions, got {len(activation_functions)}\")\n",
        "\n",
        "        # Convert activation function names to actual functions\n",
        "        self.activation_functions = []\n",
        "        for func_name in activation_functions:\n",
        "            if func_name not in ACTIVATION_FUNCTIONS:\n",
        "                raise ValueError(f\"Unknown activation function: {func_name}\")\n",
        "            self.activation_functions.append(ACTIVATION_FUNCTIONS[func_name])\n",
        "\n",
        "        # Initialize weight matrices and bias vectors\n",
        "        # These will be set by PSO during optimization\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        # Create placeholders for weights and biases between each layer\n",
        "        for i in range(self.num_layers - 1):\n",
        "            # Weight matrix from layer i to layer i+1\n",
        "            # Shape: (neurons_in_layer_i, neurons_in_layer_i+1)\n",
        "            weight_matrix = np.zeros((layer_sizes[i], layer_sizes[i + 1]))\n",
        "            self.weights.append(weight_matrix)\n",
        "\n",
        "            # Bias vector for layer i+1\n",
        "            # Shape: (neurons_in_layer_i+1,)\n",
        "            bias_vector = np.zeros(layer_sizes[i + 1])\n",
        "            self.biases.append(bias_vector)\n",
        "\n",
        "    def set_parameters(self, parameter_vector: np.ndarray):\n",
        "        \"\"\"\n",
        "        Set all weights and biases from a flat 1D parameter vector\n",
        "\n",
        "        This method is called by PSO to decode a particle's position\n",
        "        into the ANN's weights and biases\n",
        "\n",
        "        Args:\n",
        "            parameter_vector: 1D numpy array containing all parameters\n",
        "                            Order: weights for layer 0→1, biases for layer 1,\n",
        "                                   weights for layer 1→2, biases for layer 2, etc.\n",
        "        \"\"\"\n",
        "        index = 0\n",
        "\n",
        "        # Extract and set weights for each layer connection\n",
        "        for i in range(len(self.weights)):\n",
        "            # Calculate how many weights connect layer i to layer i+1\n",
        "            weight_count = self.layer_sizes[i] * self.layer_sizes[i + 1]\n",
        "\n",
        "            # Extract the weights and reshape into matrix form\n",
        "            self.weights[i] = parameter_vector[index:index + weight_count].reshape(\n",
        "                self.layer_sizes[i], self.layer_sizes[i + 1]\n",
        "            )\n",
        "            index += weight_count\n",
        "\n",
        "        # Extract and set biases for each layer (except input layer)\n",
        "        for i in range(len(self.biases)):\n",
        "            # Number of biases = number of neurons in target layer\n",
        "            bias_count = self.layer_sizes[i + 1]\n",
        "\n",
        "            # Extract the biases\n",
        "            self.biases[i] = parameter_vector[index:index + bias_count]\n",
        "            index += bias_count\n",
        "\n",
        "    def get_parameter_count(self) -> int:\n",
        "        \"\"\"\n",
        "        Calculate total number of parameters (weights + biases) in the network\n",
        "\n",
        "        This determines the dimensionality of the PSO search space\n",
        "\n",
        "        Returns:\n",
        "            Integer count of total parameters\n",
        "        \"\"\"\n",
        "        count = 0\n",
        "\n",
        "        # Count weights and biases for each layer connection\n",
        "        for i in range(self.num_layers - 1):\n",
        "            # Weights between layer i and i+1\n",
        "            count += self.layer_sizes[i] * self.layer_sizes[i + 1]\n",
        "            # Biases for layer i+1\n",
        "            count += self.layer_sizes[i + 1]\n",
        "\n",
        "        return count\n",
        "\n",
        "    def forward(self, inputs: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Perform forward propagation through the network\n",
        "\n",
        "        Computes output by passing inputs through all layers:\n",
        "        For each layer: z = W^T * a + b, then a_next = activation(z)\n",
        "\n",
        "        Args:\n",
        "            inputs: Input vector (1D numpy array)\n",
        "                   Shape: (n_features,)\n",
        "\n",
        "        Returns:\n",
        "            Output vector from final layer\n",
        "            Shape: (n_outputs,)\n",
        "        \"\"\"\n",
        "        # Start with input layer activation\n",
        "        activation = inputs\n",
        "\n",
        "        # Propagate through each layer\n",
        "        for i in range(len(self.weights)):\n",
        "            # Linear transformation: z = a * W + b\n",
        "            # where a is activation from previous layer\n",
        "            z = np.dot(activation, self.weights[i]) + self.biases[i]\n",
        "\n",
        "            # Apply non-linear activation function\n",
        "            activation = self.activation_functions[i](z)\n",
        "\n",
        "        # Return final layer output\n",
        "        return activation\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Make predictions for multiple input samples\n",
        "\n",
        "        Args:\n",
        "            X: Input matrix of shape (n_samples, n_features)\n",
        "\n",
        "        Returns:\n",
        "            Predictions of shape (n_samples, n_outputs)\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "\n",
        "        # Process each sample individually\n",
        "        for sample in X:\n",
        "            prediction = self.forward(sample)\n",
        "            predictions.append(prediction)\n",
        "\n",
        "        return np.array(predictions)"
      ]
    }
  ]
}